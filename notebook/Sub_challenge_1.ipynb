{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dask import compute, persist\n",
    "from dask.distributed import Client, progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split #,KFold,RandomizedSearchCV, GridSearchCV, HyperbandSearchCV #, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import dask_xgboost as dxgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.model_selection import KFold,RandomizedSearchCV , GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score,matthews_corrcoef\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask_jobqueue\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# cluster = SLURMCluster(cores=32,\n",
    "#                        processes=16,\n",
    "#                        memory=\"128GB\",\n",
    "#                        walltime=\"24:00:00\",\n",
    "#                        queue=\"workq\",\n",
    "#                        interface=\"ipogif0\",\n",
    "\n",
    "#                        project=\"k1423\"\n",
    "#                       )\n",
    "# print(cluster.job_script())\n",
    "# cluster.scale(jobs=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(cluster)\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184B5.csv\n",
      "BT483.csv\n",
      "HCC1428.csv\n",
      "HCC1806.csv\n",
      "HCC202.csv\n",
      "Hs578T.csv\n",
      "MCF12A.csv\n",
      "MDAMB231.csv\n",
      "MDAMB468.csv\n",
      "SKBR3.csv\n",
      "UACC3199.csv\n",
      "ZR751.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_files = [x for x in os.listdir(\"data/EDITED_CELL_LINES_FILES_COMPLETE/\") if \".csv\" in x and x != \"subchallenge_1_template_data.csv\"  ]\n",
    "# files = ['184B5.csv',\n",
    "# 'BT483.csv',\n",
    "# 'HCC1428.csv',\n",
    "# 'HCC1806.csv',\n",
    "# 'HCC202.csv',\n",
    "# 'Hs578T.csv',\n",
    "# 'MCF12A.csv',\n",
    "# 'MDAMB231.csv',\n",
    "# 'MDAMB468.csv',\n",
    "# 'SKBR3.csv',\n",
    "# 'UACC3199.csv',\n",
    "# 'ZR751.csv']\n",
    "#target cell lines subch one\n",
    "target_cell_lines = ['AU565', 'EFM19', 'HCC2218', 'LY2', 'MACLS2', 'MDAMB436']\n",
    "\n",
    "#targert genes subchallenge one\n",
    "target_genes = ['p.ERK', 'p.Akt.Ser473.', 'p.S6', 'p.PLCg2_c', 'p.HER2_c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "for m in all_files:\n",
    "#     print (x)\n",
    "    df_temp = dd.read_csv(\"data/EDITED_CELL_LINES_FILES_COMPLETE/%s\"%(m))\n",
    "    df_temp.drop(['Unnamed: 0', 'Unnamed: 0.1'],axis=1)\n",
    "#     df_temp = dd.read_csv(\"data/%s\"%(x) )  #,dtype=dtype)\n",
    "    df.append(df_temp)\n",
    "    \n",
    "for x in os.listdir(\"data\"):\n",
    "#     print (x)\n",
    "    df_temp = dd.read_csv(\"data/%s\"%(x))\n",
    "#     df_temp.drop(['Unnamed: 0', 'Unnamed: 0.1'],axis=1)\n",
    "#     df_temp = dd.read_csv(\"data/%s\"%(x) )  #,dtype=dtype)\n",
    "    df.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.concat(df,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling NA\n"
     ]
    }
   ],
   "source": [
    "print (\"filling NA\")\n",
    "# y = y.compute()\n",
    "with ProgressBar():\n",
    "    for m in [\"p.HER2\",\"p.PLCg2\"] :\n",
    "#         print (df[m])\n",
    "        \n",
    "        ddf[\"%s_c\"%(m)] =ddf[m].fillna(ddf[m].mean() )#, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.get_dummies(ddf.categorize()).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = ddf.drop([\"p.HER2\",\"p.PLCg2\",\"cellID\",\"fileID\"],axis=1)\n",
    "ddf = ddf.drop([\"p.HER2\",\"p.PLCg2\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounds = {}\n",
    "# genes= ['b.CATENIN','cleavedCas','CyclinB','GAPDH','IdU','Ki.67','p.4EBP1','p.Akt.Ser473.', 'p.AKT.Thr308.',\n",
    "#         'p.AMPK','p.BTK','p.CREB','p.ERK','p.FAK','p.GSK3b','p.H3','p.HER2_c','p.JNK','p.MAP2K3',\n",
    "#         'p.MAPKAPK2','p.MEK','p.MKK3.MKK6','p.MKK4','p.NFkB','p.p38','p.p53','p.p90RSK','p.PDPK1',\n",
    "#         'p.PLCg2_c','p.RB','p.S6','p.S6K','p.SMAD23','p.SRC','p.STAT1','p.STAT3','p.STAT5']\n",
    "\n",
    "# for x in range(len (genes)):\n",
    "# #     print (x)\n",
    "#     my_list = ['b.CATENIN','cleavedCas','CyclinB','GAPDH','IdU','Ki.67','p.4EBP1','p.Akt.Ser473.',\n",
    "#                'p.AKT.Thr308.','p.AMPK','p.BTK','p.CREB','p.ERK','p.FAK','p.GSK3b','p.H3','p.HER2_c','p.JNK',\n",
    "#                'p.MAP2K3','p.MAPKAPK2','p.MEK',   'p.MKK3.MKK6','p.MKK4','p.NFkB','p.p38','p.p53','p.p90RSK',\n",
    "#                'p.PDPK1','p.PLCg2_c','p.RB','p.S6','p.S6K','p.SMAD23','p.SRC','p.STAT1','p.STAT3','p.STAT5']\n",
    "\n",
    "#     if x+1 <len (genes):\n",
    "# #         my_list = genes \n",
    "# #         print (x,genes[x], genes[x+1])\n",
    "#         my_list.pop(x)\n",
    "# #         my_list.pop(x)\n",
    "# #         my_list.pop(x+1)\n",
    "# #         print ( my_list )\n",
    "# #         rounds[x] = { \"test\":[genes[x], genes[x+1]] , \"train\":my_list}\n",
    "#         rounds[x] = { \"test\":[genes[x] ], \"train\":my_list}\n",
    "# #         y = df[[genes[x], genes[x+1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats=  ['time','treatment_EGF', 'treatment_full', 'treatment_iEGFR', 'treatment_iPI3K','treatment_iPKC', 'treatment_iMEK']\n",
    "\n",
    "# for k in  rounds :\n",
    "#     for ff in feats: \n",
    "#         rounds[k][\"train\"].append(ff)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for sub challenge one \n",
    "train_ddf = ddf[~ddf[\"cell_line\"].isin(target_cell_lines)]\n",
    "test_ddf = ddf[ddf[\"cell_line\"].isin(target_cell_lines)]\n",
    "X_train = train_ddf.drop(target_genes,axis=1)\n",
    "X_train = X_train.to_dask_array(lengths=True)\n",
    "y_train = train_ddf[target_genes]\n",
    "X_test = test_ddf.drop(target_genes,axis=1)\n",
    "X_test = X_test.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_target_ditionary = {'ERK':'p.ERK', 'AKT':'p.Akt.Ser473.', 'S6':'p.S6', 'HER':'p.HER2_c', 'PLCG2':'p.PLCg2_c'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in my_target_ditionary.keys():\n",
    "    \n",
    "    y2 = y_train[my_target_ditionary[key]]\n",
    "    \n",
    "    y2 = y2.to_dask_array(lengths=True)\n",
    "    print (\"split\")\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,y2,test_size=0.3, random_state=101)\n",
    "    \n",
    "    print (\"scaling\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    base_model = dxgb.XGBRegressor(objective='reg:squarederror',tree_method='hist',\n",
    "                               verbosity=3,n_jobs=-1,n_estimators=1000,\n",
    "                               learning_rate=0.1  ,max_depth=0,max_leaves=64,grow_policy='lossguide') \n",
    "    with joblib.parallel_backend('dask'):\n",
    "        base_model.fit(X_train, y_train.flatten())\n",
    "        base_model.save_model('%s_xgboost.model'%(key))\n",
    "\n",
    "    predictions = base_model.predict( X_test)\n",
    "    predictions = client.persist(predictions)\n",
    "\n",
    "    p = predictions.to_dask_dataframe(columns=y.columns)\n",
    "    p.to_csv(\"my_result_%s_subOne\"%(my_target_ditionary[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = ddf[rounds[0][\"train\"]]\n",
    "# y = ddf[rounds[0][\"test\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"scaling\")\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = dxgb.XGBRegressor(objective='reg:squarederror',tree_method='hist',\n",
    "                               verbosity=3,n_jobs=-1,n_estimators=1000,\n",
    "                               learning_rate=0.1  ,max_depth=0,max_leaves=64,grow_policy='lossguide' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with joblib.parallel_backend('dask'):\n",
    "#     base_model.fit(X_train, y_train.flatten())\n",
    "# # base_model.save_model('base_line_no_max_deph_lr_%f_%i.model'%(lr,leaves))\n",
    "\n",
    "# predictions = base_model.predict( X_test)\n",
    "# predictions = client.persist(predictions)\n",
    "# print (\"########\")\n",
    "# # print (\"lr :%f , leaves:%i \" %(lr,leaves))\n",
    "# print (\"R^2:\",r2_score(y_test.compute(), predictions.compute()))\n",
    "# print (\"MAE:\",mean_absolute_error(y_test.compute(), predictions.compute()))\n",
    "# print (\"MSE:\",mean_squared_error(y_test.compute(), predictions.compute()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.Booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxgb.core.xgb.Booster(model_file=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
